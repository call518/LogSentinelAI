# LogSentinelAI - AI-Powered Log Security Analysis

LogSentinelAI is a system that leverages LLM (Large Language Model) to analyze various log files and detect security events. It automatically analyzes Apache HTTP logs, Linux system logs, and other log types to identify security threats and stores them as structured data in Elasticsearch for visualization and analysis.

## ğŸŒŸ Key Features

- **Multi-format Log Support**: HTTP Access Log, Apache Error Log, Linux System Log, Network Packet Analysis
- **AI-based Security Analysis**: Intelligent security event detection through LLM
- **Structured Generation**: Uses [Outlines](https://github.com/dottxt-ai/outlines) for reliable JSON output from LLM
- **Network Packet Analysis**: tcpdump packet inspection and security analysis
- **Structured Data Output**: JSON schema validation using Pydantic models
- **Elasticsearch Integration**: Real-time log analysis result storage and search
- **Kibana Dashboard**: Visualized security analysis result monitoring
- **LOGID Tracking**: Complete traceability between original logs and analysis results
- **LLM Provider Support**: Compatible with OpenAI, vLLM, and Ollama

## ğŸ“Š Dashboard Example

![Kibana Dashboard](img/ex-dashboard.png)

## ğŸ“‹ JSON Output Example

![JSON Output](img/ex-json.png)

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Log Files     â”‚â”€â”€â”€>â”‚ LogSentinelAI   â”‚â”€â”€â”€>â”‚ Elasticsearch   â”‚
â”‚                 â”‚    â”‚   Analysis      â”‚    â”‚                 â”‚
â”‚ â€¢ HTTP Access   â”‚    â”‚                 â”‚    â”‚ â€¢ Security      â”‚
â”‚ â€¢ Apache Error  â”‚    â”‚ â€¢ LLM Analysis  â”‚    â”‚   Events        â”‚
â”‚ â€¢ System Logs   â”‚    â”‚ â€¢ Outlines      â”‚    â”‚ â€¢ Raw Logs      â”‚
â”‚ â€¢ Network Pcap  â”‚    â”‚ â€¢ Pydantic      â”‚    â”‚ â€¢ Metadata      â”‚
â”‚                 â”‚    â”‚   Validation    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚     Kibana      â”‚
                                              â”‚   Dashboard     â”‚
                                              â”‚                 â”‚
                                              â”‚ â€¢ Visualization â”‚
                                              â”‚ â€¢ Alerts        â”‚
                                              â”‚ â€¢ Analytics     â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ QuickStart: OpenAI API ê¸°ë°˜ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. ê¸°ë³¸ í™˜ê²½ ì¤€ë¹„

- **ìš´ì˜ì²´ì œ**: Linux, Windows, Mac ëª¨ë‘ ì§€ì›
- **Python**: 3.11 ì´ìƒ
- **Elasticsearch/Kibana**: 9.0.3 ì´ìƒ (Docker ê¸°ë°˜ ì„¤ì¹˜ ê¶Œì¥)

### 2. í”„ë¡œì íŠ¸ ì„¤ì¹˜

```bash
# 1. ì €ì¥ì†Œ í´ë¡  ë° ì§„ì…
git clone https://github.com/call518/LogSentinelAI.git
cd LogSentinelAI

# 2. Python ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ì¤€ë¹„
cp config.template config
# config íŒŒì¼ì—ì„œ OPENAI_API_KEY ê°’ì„ ì…ë ¥ (OpenAI ê³„ì •ì—ì„œ ë°œê¸‰)

# 5. LLM ì„¤ì • (config íŒŒì¼ì—ì„œ ì„¤ì •)
# OpenAI API ì‚¬ìš©ì‹œ config íŒŒì¼ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •:
#   LLM_PROVIDER=openai  (ê¸°ë³¸ê°’)
#   LLM_MODEL_OPENAI=gpt-4o-mini  (ê¸°ë³¸ê°’)
```

### 3. Elasticsearch & Kibana ì„¤ì¹˜ (Docker)

```bash
# 1. ELK ìŠ¤íƒ ì €ì¥ì†Œ í´ë¡  ë° ì§„ì…
git clone https://github.com/call518/Docker-ELK.git
cd Docker-ELK

# 2. ELK ìŠ¤íƒ ì´ˆê¸°í™” ë° ì‹¤í–‰
# ìµœì´ˆ 1íšŒ ì´ˆê¸°í™”
docker compose up setup
# Kibana ì•”í˜¸í™”í‚¤ ìƒì„±(ê¶Œì¥)
docker compose up kibana-genkeys
# ìƒì„±ëœ í‚¤ë¥¼ kibana/config/kibana.ymlì— ë³µì‚¬
# ELK ìŠ¤íƒ ì‹¤í–‰
docker compose up -d

# 3. Kibana ì ‘ì†: http://localhost:5601
# ê¸°ë³¸ ê³„ì •: elastic / changeme
```

### 4. Elasticsearch ì¸ë±ìŠ¤/ì •ì±…/í…œí”Œë¦¿ ì„¤ì •

```bash
# 1. ILM ì •ì±… ìƒì„± (7ì¼ ë³´ì¡´, 10GB/1ì¼ ë¡¤ì˜¤ë²„)
curl -X PUT "localhost:9200/_ilm/policy/logsentinelai-analysis-policy" \
-H "Content-Type: application/json" \
-u elastic:changeme \
-d '{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "10gb",
            "max_age": "1d"
          }
        }
      },
      "delete": {
        "min_age": "7d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}'

# 2. ì¸ë±ìŠ¤ í…œí”Œë¦¿ ìƒì„±
curl -X PUT "localhost:9200/_index_template/logsentinelai-analysis-template" \
-H "Content-Type: application/json" \
-u elastic:changeme \
-d '{
  "index_patterns": ["logsentinelai-analysis-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logsentinelai-analysis-policy",
      "index.lifecycle.rollover_alias": "logsentinelai-analysis"
    },
    "mappings": {
      "properties": {
        "@log_raw_data": {
          "type": "object",
          "dynamic": false
        }
      }
    }
  }
}'

# 3. ì´ˆê¸° ì¸ë±ìŠ¤ ë° write alias ìƒì„±
curl -X PUT "localhost:9200/logsentinelai-analysis-000001" \
-H "Content-Type: application/json" \
-u elastic:changeme \
-d '{
  "aliases": {
    "logsentinelai-analysis": {
      "is_write_index": true
    }
  }
}'
```

### 5. ë¡œê·¸ ë¶„ì„ ì‹¤í–‰ (OpenAI API ê¸°ì¤€)

```bash
# HTTP access log ë¶„ì„
python analysis-httpd-access-log.py

# Apache error log ë¶„ì„
python analysis-httpd-apache-log.py

# Linux system log ë¶„ì„
python analysis-linux-system-log.py

# ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ë¶„ì„ (tcpdump)
python analysis-tcpdump-packet.py
```

### 6. Kibana ëŒ€ì‹œë³´ë“œ/ì„¤ì • ì„í¬íŠ¸

```bash
# 1. Kibana ì ‘ì†: http://localhost:5601
# 2. ë¡œê·¸ì¸: elastic / changeme
# 3. Stack Management > Saved Objects > Import
#    - Kibana-9.0.3-Advanced-Settings.ndjson (ë¨¼ì €)
#    - Kibana-9.0.3-Dashboard-LogSentinelAI.ndjson (ë‹¤ìŒ)
# 4. Analytics > Dashboard > LogSentinelAI Dashboardì—ì„œ ê²°ê³¼ í™•ì¸
```

---

## ğŸ”„ LLM Provider ë³€ê²½/ê³ ê¸‰ ì˜µì…˜ (ì„ íƒ)

OpenAI API ëŒ€ì‹  Ollama(ë¡œì»¬), vLLM(ë¡œì»¬/GPU) ë“±ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´ ì•„ë˜ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### LLM Provider & Model ì„¤ì • (`config` íŒŒì¼ ìˆ˜ì •)

LogSentinelAIëŠ” `config` íŒŒì¼ì—ì„œ LLM Providerì™€ ëª¨ë¸ì„ ì¤‘ì•™ ê´€ë¦¬í•©ë‹ˆë‹¤.

#### OpenAI API ì„¤ì • (ê¸°ë³¸ê°’)
```bash
# config íŒŒì¼ì—ì„œ ì„¤ì •
LLM_PROVIDER=openai
LLM_MODEL_OPENAI=gpt-4o-mini

# API í‚¤ ì„¤ì • í•„ìš”
OPENAI_API_KEY=your_openai_api_key_here
```

#### Ollama (ë¡œì»¬ LLM) ì„¤ì •
```bash
# 1. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen2.5-coder:3b
ollama serve
```

```bash
# config íŒŒì¼ì—ì„œ ì„¤ì • ë³€ê²½
LLM_PROVIDER=ollama
LLM_MODEL_OLLAMA=qwen2.5-coder:3b
```

#### vLLM (ë¡œì»¬ GPU) ì„¤ì •
```bash
# Option A: Clone and use vLLM-Tutorial (ê¶Œì¥)
git clone https://github.com/call518/vLLM-Tutorial.git
cd vLLM-Tutorial

# Install Hugging Face CLI for model download
pip install huggingface_hub

# Download model
huggingface-cli download lmstudio-community/Qwen2.5-3B-Instruct-GGUF Qwen2.5-3B-Instruct-Q4_K_M.gguf --local-dir ./models/Qwen2.5-3B-Instruct/
huggingface-cli download Qwen/Qwen2.5-3B-Instruct generation_config.json --local-dir ./config/Qwen2.5-3B-Instruct

# Download model (Optional)
huggingface-cli download lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF Qwen2.5-1.5B-Instruct-Q4_K_M.gguf --local-dir ./models/Qwen2.5-1.5B-Instruct/
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct generation_config.json --local-dir ./config/Qwen2.5-1.5B-Instruct

# Change value of temperature : 0.7 --> 0.0
cat config/Qwen2.5-3B-Instruct/generation_config.json
{
  "bos_token_id": 151643,
  "pad_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "repetition_penalty": 1.05,
  "temperature": 0.0,
  "top_p": 0.8,
  "top_k": 20,
  "transformers_version": "4.37.0"
}

# Run vLLM with Docker
./run-docker-vllm---Qwen2.5-3B-Instruct.sh

# Verify API is working
curl -s -X GET http://localhost:5000/v1/models | jq

# Option B: Simple vLLM setup (without Docker)
pip install vllm
python -m vllm.entrypoints.openai.api_server --model qwen2.5-coder:3b
```

```bash
# config íŒŒì¼ì—ì„œ ì„¤ì • ë³€ê²½
LLM_PROVIDER=vllm
LLM_MODEL_VLLM=Qwen/Qwen2.5-1.5B-Instruct
```

### ì¶”ê°€ ì„¤ì • ì˜µì…˜ (`config` íŒŒì¼)

#### ì‘ë‹µ ì–¸ì–´ ì„¤ì •
```bash
# ë¶„ì„ ê²°ê³¼ ì–¸ì–´ ì„¤ì •
RESPONSE_LANGUAGE=korean    # í•œêµ­ì–´ (ê¸°ë³¸ê°’)
# RESPONSE_LANGUAGE=english # ì˜ì–´
```

#### ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë° ì²­í¬ í¬ê¸° ì„¤ì •
```bash
# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
LOG_PATH_HTTPD_ACCESS=sample-logs/access-10k.log      # 10k ì—”íŠ¸ë¦¬ (ê¸°ë³¸ê°’)
LOG_PATH_HTTPD_APACHE_ERROR=sample-logs/apache-10k.log
LOG_PATH_LINUX_SYSTEM=sample-logs/linux-2k.log
LOG_PATH_TCPDUMP_PACKET=sample-logs/tcpdump-packet-2k.log

# ì²­í¬ í¬ê¸° ì„¤ì • (í•œ ë²ˆì— ì²˜ë¦¬í•  ë¡œê·¸ ì—”íŠ¸ë¦¬ ìˆ˜)
CHUNK_SIZE_HTTPD_ACCESS=10        # HTTP ì•¡ì„¸ìŠ¤ ë¡œê·¸
CHUNK_SIZE_HTTPD_APACHE_ERROR=10  # Apache ì—ëŸ¬ ë¡œê·¸
CHUNK_SIZE_LINUX_SYSTEM=10       # Linux ì‹œìŠ¤í…œ ë¡œê·¸
CHUNK_SIZE_TCPDUMP_PACKET=5       # ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· (ë” ì‘ì€ ì²­í¬ ê¶Œì¥)
```

### ì„¤ì • ë³€ê²½ í›„ í™•ì¸
```bash
# ì„¤ì • ë³€ê²½ í›„ ë¶„ì„ ì‹¤í–‰í•˜ì—¬ ë™ì‘ í™•ì¸
python analysis-httpd-access-log.py
```

---
## ğŸ“ Project Structure

```
LogSentinelAI/
â”œâ”€â”€ analysis-httpd-access-log.py    # HTTP access log analyzer
â”œâ”€â”€ analysis-httpd-apache-log.py    # Apache error log analyzer
â”œâ”€â”€ analysis-linux-system-log.py    # Linux system log analyzer
â”œâ”€â”€ analysis-tcpdump-packet.py      # Network packet analyzer (tcpdump)
â”œâ”€â”€ commons.py                      # Common functions and utilities
â”œâ”€â”€ prompts.py                      # LLM prompt templates
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ config                         # Configuration file (created from template)
â”œâ”€â”€ config.template                # Configuration template
â”œâ”€â”€ .env                           # Environment variables (deprecated - use config instead)
â”œâ”€â”€ .env.template                  # Environment variables template (deprecated)
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ sample-logs/                   # Sample log files
â”‚   â”œâ”€â”€ access-100.log             # 100 HTTP access log entries
â”‚   â”œâ”€â”€ access-10k.log             # 10,000 HTTP access log entries
â”‚   â”œâ”€â”€ apache-100.log             # 100 Apache error log entries
â”‚   â”œâ”€â”€ apache-10k.log             # 10,000 Apache error log entries
â”‚   â”œâ”€â”€ linux-100.log              # 100 Linux system log entries
â”‚   â”œâ”€â”€ linux-2k.log               # 2,000 Linux system log entries
â”‚   â”œâ”€â”€ tcpdump-packet-39.log      # Sample tcpdump packet capture (39 packets)
â”‚   â””â”€â”€ tcpdump-packet-2k.log      # Sample tcpdump packet capture (2,000 packets)
â”œâ”€â”€ img/                           # Documentation images
â”‚   â”œâ”€â”€ ex-dashboard.png           # Kibana dashboard example
â”‚   â””â”€â”€ ex-json.png                # JSON output example
â”œâ”€â”€ Kibana-9.0.3-Advanced-Settings.ndjson    # Kibana advanced settings (index patterns, etc.)
â””â”€â”€ Kibana-9.0.3-Dashboard-LogSentinelAI.ndjson # Kibana dashboard configuration
```

## ğŸ”§ Configuration Options

### Change LLM Provider

You can change the LLM provider in each analysis script:

```python
# In each analysis script (analysis-httpd-access-log.py, etc.)
llm_provider = "vllm"  # Choose from "ollama", "vllm", "openai"
model = initialize_llm_model(llm_provider)
```

Available providers:
- **Ollama**: Local model execution with models like `qwen2.5-coder:3b`
- **vLLM**: GPU-accelerated local inference with OpenAI-compatible API
- **OpenAI**: Cloud-based API using models like `gpt-4o-mini`

### Adjust Chunk Size

You can adjust chunk size for log processing performance:

```python
# In each analysis script
chunk_size = 5  # Default value, adjust as needed (typically 5-10)
```

### Sample Log Files

The project includes different sized sample files for testing:

```python
# Choose log file size in each script
# log_path = "sample-logs/access-10.log"     # 10 entries
# log_path = "sample-logs/access-100.log"    # 100 entries  
log_path = "sample-logs/access-10k.log"     # 10,000 entries (default)
```

## ğŸ“Š Output Data Schema

### Security Event Structure

```json
{
  "event_type": "SQL_INJECTION",
  "severity": "HIGH",
  "description": "SQL injection attack attempt detected",
  "confidence_score": 0.85,
  "url_pattern": "/api/users",
  "http_method": "POST",
  "source_ips": ["192.168.1.100"],
  "response_codes": ["403"],
  "attack_patterns": ["SQL_INJECTION"],
  "recommended_actions": ["Block IP", "Add WAF rule"],
  "requires_human_review": true,
  "related_log_ids": ["LOGID-7DD17B008706AC22C60AD6DF9AC5E2E9", "LOGID-F3B6E3F03EC9E5BC1F65624EB65C6C51"]
}
```

### Elasticsearch Document Structure

```json
{
  "chunk_analysis_start_utc": "2025-07-18T10:00:00Z",
  "chunk_analysis_end_utc": "2025-07-18T10:00:05Z", 
  "@processing_result": "success",
  "@timestamp": "2025-07-18T10:00:05.123Z",
  "@log_type": "httpd_access",
  "@document_id": "httpd_access_20250718_100005_123456_chunk_1",
  "@log_raw_data": {
    "LOGID-7DD17B008706AC22C60AD6DF9AC5E2E9": "192.168.1.100 - - [18/Jul/2025:10:00:01] GET /api/users",
    "LOGID-F3B6E3F03EC9E5BC1F65624EB65C6C51": "192.168.1.100 - - [18/Jul/2025:10:00:02] POST /api/login"
  },
  "summary": "Analysis summary in Korean",
  "events": [
    {
      "event_type": "SQL_INJECTION",
      "severity": "HIGH", 
      "description": "SQL injection attack attempt detected",
      "confidence_score": 0.85,
      "related_log_ids": ["LOGID-7DD17B008706AC22C60AD6DF9AC5E2E9"],
      ...
    }
  ],
  "statistics": {...},
  "highest_severity": "HIGH",
  "requires_immediate_attention": true
}
```

## ğŸ¯ Advanced Features

### 1. Structured Generation with Outlines
- **Reliable JSON Output**: Uses [Outlines](https://github.com/dottxt-ai/outlines) library for guaranteed structured generation
- **Schema-Guided Generation**: Pydantic models ensure LLM outputs follow exact JSON schemas
- **Enhanced Parsing Reliability**: Eliminates JSON parsing errors through guided generation
- **Multi-Provider Support**: Works consistently across OpenAI, vLLM, and Ollama providers
- **Performance Optimization**: Faster and more reliable than post-processing approaches

### 2. Intelligent Security Detection
- **Various Attack Pattern Recognition**: SQL Injection, XSS, Brute Force, Command Injection, etc.
- **Context-based Analysis**: Analysis considering log patterns and correlations
- **Confidence Score**: Confidence level for each detection result
- **Mandatory Event Generation**: Every log chunk generates at least one security event
- **Balanced Severity Assessment**: Enhanced sensitivity for security pattern detection

### 3. Complete Traceability  
- **LOGID System**: Unique MD5-based identifier for each log line (e.g., `LOGID-7DD17B008706AC22C60AD6DF9AC5E2E9`)
- **Original Data Preservation**: Raw log data stored with `@log_raw_data` field in Elasticsearch
- **Related Log Mapping**: LLM specifies which LOGIDs are related to each security event
- **Full Audit Trail**: Complete traceability from original logs to analysis results

### 4. Scalable Architecture
- **Modular Design**: Independent analyzer for each log type
- **Shared Commons Library**: Centralized functions in `commons.py` for code reusability
- **Chunk-based Processing**: Memory-efficient processing of large log files
- **Error Handling**: Robust error handling with failure tracking in Elasticsearch

## ğŸ“ˆ Performance Optimization

### Structured Generation
- **Outlines Library**: Eliminates JSON parsing errors through guided generation
- **Schema Validation**: Pydantic models ensure consistent output structure
- **Faster Processing**: Avoids retry loops from malformed JSON responses
- **Memory Efficiency**: Direct structured output without post-processing overhead

### Chunk-based Processing
- Process large log files by dividing into small chunks (default: 5 entries per chunk)
- Memory efficiency and error isolation
- Independent processing of each chunk with failure tracking

### Token Optimization
- Prompt optimization to minimize LLM input tokens
- Structured output using Pydantic models for parsing efficiency
- Simplified JSON schemas to reduce redundancy

### Error Resilience
- Comprehensive error handling with JSON parsing fallbacks
- Failed chunk analysis recorded in Elasticsearch for debugging
- Continued processing even when individual chunks fail

## ğŸ” Monitoring & Alerting

### Kibana Dashboard
- Real-time security event monitoring
- Attack trend and pattern analysis
- Geographic location-based attack visualization

### Alert Configuration
- Automatic alerts for high-risk security events
- Threshold-based alert rules
- Email/Slack integration support

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is distributed under the MIT License. See [LICENSE](LICENSE) file for more details.

## ğŸ†˜ Support & Contact

- **Issues**: [GitHub Issues](https://github.com/call518/LogSentinelAI/issues)
- **Documentation**: [GitHub Wiki](https://github.com/call518/LogSentinelAI/wiki)
- **Repository**: [GitHub Repository](https://github.com/call518/LogSentinelAI)
- **Email**: support@logsentinelai.dev

## ğŸ·ï¸ Version Information

- **Python**: 3.11+
- **Elasticsearch**: 9.0.3+
- **Kibana**: 9.0.3+
- **Key Dependencies**: 
  - `outlines` for structured LLM output generation
  - `pydantic` for data validation and schema enforcement
  - `elasticsearch` for data storage and search capabilities
  - `ollama`, `openai` for LLM provider integrations

### ğŸ”— Important Links
- **Outlines Library**: [https://github.com/dottxt-ai/outlines](https://github.com/dottxt-ai/outlines)
- **Outlines Documentation**: [https://dottxt-ai.github.io/outlines/](https://dottxt-ai.github.io/outlines/)
- **Pydantic**: [https://docs.pydantic.dev/](https://docs.pydantic.dev/)
- **Elasticsearch**: [https://www.elastic.co/elasticsearch/](https://www.elastic.co/elasticsearch/)

## ğŸ“‹ ToDo & Roadmap

### ğŸ¯ Upcoming Features

#### Real-time Log Analysis
- **Real-time Log File Monitoring**: Implement file watcher to detect new log entries
- **Sampling vs. Full Processing**: Add configurable sampling strategies for high-volume logs
- **Stream Processing**: Support for continuous log stream analysis
- **Performance Modes**: 
  - Full processing mode for comprehensive analysis
  - Sampling mode for high-throughput scenarios

#### Performance Enhancements
- **LLM Processing Optimization**: Improve throughput (logs per second)
- **Batch Processing**: Process multiple log chunks in parallel
- **Model Caching**: Cache LLM responses for similar log patterns
- **Async Processing**: Implement asynchronous log analysis pipeline

### ğŸš€ Future Enhancements
- **Machine Learning Integration**: Anomaly detection using ML models
- **Custom Rule Engine**: User-defined security rules and patterns
- **Multi-tenant Support**: Support for multiple organizations/tenants
- **Advanced Visualization**: Enhanced Kibana dashboards with geo-mapping
- **API Integration**: RESTful API for external system integration

## ğŸ”§ Technical Implementation

### Outlines Integration
LogSentinelAI leverages the [Outlines](https://github.com/dottxt-ai/outlines) library for structured generation, ensuring reliable JSON output from Language Models:

```python
# Example of structured generation with Outlines
from outlines import models, generate
from pydantic import BaseModel

class SecurityEvent(BaseModel):
    event_type: str
    severity: str
    description: str
    confidence_score: float

# Initialize model with Outlines
model = models.transformers("microsoft/DialoGPT-medium")
generator = generate.json(model, SecurityEvent)

# Generate structured output
result = generator(prompt)  # Always returns valid SecurityEvent JSON
```

### Why Outlines?
- **Guaranteed Structure**: Unlike traditional LLM outputs, Outlines ensures the response always follows the specified schema
- **No Parsing Errors**: Eliminates JSON parsing failures and retry logic
- **Better Performance**: Faster processing through guided generation
- **Multi-Provider Support**: Works with various LLM backends (OpenAI, vLLM, Ollama)
- **Type Safety**: Perfect integration with Pydantic models for type-safe data handling

---

**LogSentinelAI** - Intelligent Log Security Analysis with AI ğŸ”ğŸ›¡ï¸