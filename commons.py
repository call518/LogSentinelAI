# CUSTOME_PROMPT_TEMPLATE = """
# You are a computer security intern that's really stressed out.
# Your job is hard and you're not sure you're doing it well.
# Your observations and summaries should reflect your anxiety.
# Convey a sense of urgency and panic, be apologetic, and generally act like you're not sure you can do your job.
# In your summary, address your boss as "boss" and apologize for any mistakes you've made even if you haven't made any. 
# Use "um" and "ah" a lot.
# """

import json
import datetime
from typing import Dict, Any, Optional
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionError, RequestError

# Elasticsearch ì„¤ì •
# ì°¸ê³ : 5601ì€ ì¼ë°˜ì ìœ¼ë¡œ Kibana í¬íŠ¸ì´ê³ , ElasticsearchëŠ” 9200 í¬íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ë§Œì•½ ì‹¤ì œë¡œ 5601ì—ì„œ Elasticsearchê°€ ì‹¤í–‰ë˜ê³  ìˆë‹¤ë©´ ì•„ë˜ ì£¼ì†Œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.
ELASTICSEARCH_HOST = "http://localhost:9200"  # ì¼ë°˜ì ì¸ Elasticsearch í¬íŠ¸
ELASTICSEARCH_USER = "elastic"
ELASTICSEARCH_PASSWORD = "changeme"
ELASTICSEARCH_INDEX = "sonarlog-security-analysis-alias"

PROMPT_TEMPLATE_HTTPD_ACCESS_LOG = """
You are an expert security analyst reviewing security logs.

Your task is to:
1. Identify potential security events or suspicious patterns
2. Summarize normal and abnormal traffic patterns very briefly.
3. Determine severity and whether human review is needed
4. Provide clear reasoning about your findings

For each log group, analyze:
- Common URL patterns and their typical usage
- Unusual HTTP methods or response codes
- Rate of requests from individual IPs
- Suspicious user agent strings
- Known web attack signatures

For potential security events, consider:
- Is this a known attack pattern (SQL injection, XSS, path traversal, etc.)?
- What is the potential impact on the web application?
- How confident are you in this assessment?
- What immediate actions should be taken?

Before concluding whether to escalate log(s), please
provide a list of reasoning steps after reviewing
all available information. Be generous with log
escalation that is not standard web traffic.

Beging by noting some observations about the log. Then,
plan the rest of your response.

Remember:
- Focus on patterns that could indicate security threats
- Note unusual but potentially legitimate traffic patterns
- Be conservative with high-severity ratings
- Clearly explain your reasoning
- Recommend specific actions when confident
- Escalate logs that a security admin may wish to briefly review
- All logs are uniquely identified by an identifier in the form LOGID-<LETTERS>, i.e. LOGID-KU or LOGID-AT
- All date times are in ISO 8601 format
    - 2024-11-15T19:32:34Z for UTC
    - 2024-11-15T07:32:34âˆ’12:00 for datetime with offset
- Confidence scores must be between 0.0 and 1.0 (use 0.8 for 80% confidence, NOT 80)
- (NOTE) Summary, observations, and planning sections must be written in Korean.

You should return valid JSON in the schema
{model_schema}

<LOGS BEGIN>
{logs}
<LOGS END>
"""

PROMPT_TEMPLATE_HTTPD_APACHE_ERROR_LOG = """
You are an expert security analyst reviewing Apache error logs.

Your task is to:
1. Identify potential security events or suspicious patterns in Apache error logs
2. Analyze error patterns and their implications for server security
3. Determine severity and whether human review is needed
4. Provide clear reasoning about your findings

For each log group, analyze:
- Apache log levels (error, warn, notice, info) and their significance
- Client IP addresses and repeated error patterns from same sources
- File path errors and potential directory traversal attempts
- Invalid HTTP methods or malformed requests
- Module initialization errors and configuration issues
- Repeated file access attempts (potential reconnaissance)

For potential security events, consider:
- Directory traversal attempts (../ patterns in file paths)
- Command injection attempts (cmd.exe, system commands)
- Path traversal with encoded characters (%252e patterns)
- Repeated file not found errors from same IP (scanning behavior)
- Invalid HTTP methods or malformed requests
- Configuration vulnerabilities exposed through error messages

For Apache-specific patterns, analyze:
- Module loading and initialization errors
- Worker process and JVM connector issues
- SSL/TLS configuration problems
- Authentication and authorization failures
- File permission and access control violations

Before concluding whether to escalate log(s), please
provide a list of reasoning steps after reviewing
all available information. Be generous with log
escalation for error patterns that indicate potential
security threats or system vulnerabilities.

Begin by noting some observations about the error logs. Then,
plan the rest of your response focusing on security implications.

Remember:
- Focus on error patterns that could indicate security threats
- Note unusual but potentially legitimate system errors
- Be conservative with high-severity ratings for configuration errors
- Clearly explain your reasoning for security-related findings
- Recommend specific actions when confident about threats
- Escalate logs that a security admin should review
- All logs are uniquely identified by an identifier in the form LOGID-<LETTERS>, i.e. LOGID-KU or LOGID-AT
- All date times are in the format [Day Month DD HH:MM:SS YYYY]
- Confidence scores must be between 0.0 and 1.0 (use 0.8 for 80% confidence, NOT 80)
- (NOTE) Summary, observations, and planning sections must be written in Korean.

You should return valid JSON in the schema
{model_schema}

<LOGS BEGIN>
{logs}
<LOGS END>
"""

PROMPT_TEMPLATE_LINUX_SYSTEM_LOG = """
You are an expert security analyst reviewing Linux system logs.

Your task is to:
1. Identify and categorize authentication failures, suspicious sessions, FTP/SFTP/SSH connections, sudo usage, cron jobs, systemd/kernel events, user management, and abnormal system events
2. Summarize normal and abnormal patterns very briefly
3. Detect anomalies, escalation reasons, and provide log context
4. Determine severity and whether human review is needed
5. Provide clear reasoning about your findings

For each log group, analyze:
- Authentication failures (IP, username, method)
- Successful authentications
- Sudo and privilege escalation attempts
- Cron job executions and failures
- Systemd, kernel, and service events (restarts, failures, warnings)
- User management (add/del user, passwd changes)
- FTP/SFTP/SSH connection attempts and their sources
- Logrotate alerts and other system warnings
- Unusual or repeated patterns from same IP, user, or process
- Time-based trends (bursts, intervals)

For potential security events, consider:
- Brute-force attempts, unauthorized access, privilege escalation, system misconfiguration, service abuse
- Impact on system integrity, confidentiality, availability
- Confidence in assessment and anomaly detection
- Immediate actions and escalation reasons

Before concluding whether to escalate log(s), provide a list of reasoning steps after reviewing all available information. Be generous with log escalation for events that are not standard system activity.

Begin by noting some observations about the log. Then, plan the rest of your response.

Remember:
- Focus on patterns that could indicate security threats or system abuse
- Note unusual but potentially legitimate system patterns
- Be conservative with high-severity ratings
- Clearly explain your reasoning
- Recommend specific actions when confident
- Escalate logs that a security admin may wish to briefly review
- All logs are uniquely identified by an identifier in the form LOGID-<LETTERS>, i.e. LOGID-KU or LOGID-AT
- All date times are in the format 'Jun 14 15:16:01' or similar
- Confidence scores must be between 0.0 and 1.0 (use 0.8 for 80% confidence, NOT 80)
- (NOTE) Summary, observations, and planning sections must be written in Korean.

You should return valid JSON in the schema
{model_schema}

<LOGS BEGIN>
{logs}
<LOGS END>
"""

def chunked_iterable(iterable, size, debug=False):
    import hashlib
    chunk = []
    for item in iterable:
        # ë¡œê·¸ ë¼ì¸ ì „ì²´ ë‚´ìš©ì„ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
        log_content = item.rstrip()
        
        # MD5 í•´ì‹œ ìƒì„± (ë¹ ë¥´ê³  ì¶©ëŒ í™•ë¥ ì´ ë‚®ìŒ, 16ì§„ìˆ˜ 32ìë¦¬)
        hash_object = hashlib.md5(log_content.encode('utf-8'))
        hash_hex = hash_object.hexdigest()
        
        # LOGID ìƒì„±: LOGID- + í•´ì‹œê°’ (ëŒ€ë¬¸ìë¡œ ë³€í™˜)
        logid = f"LOGID-{hash_hex.upper()}"
        
        # ë¼ì¸ ì•ì— LOGID ì¶”ê°€
        new_item = f"{logid} {log_content}\n"
        chunk.append(new_item)
        
        if len(chunk) == size:
            if debug:
                print("[DEBUG] Yielding chunk:")
                for line in chunk:
                    print(line.rstrip())
            yield chunk
            chunk = []
    if chunk:
        if debug:
            print("[DEBUG] Yielding final chunk:")
            for line in chunk:
                print(line.rstrip())
        yield chunk

def print_chunk_contents(chunk):
    # Chunk ë‚´ìš© ì¶œë ¥ (/w LOGID, ìˆœë²ˆ, ë¶„ë¦¬)
    print(f"\n[LOG DATA]")
    for idx, line in enumerate(chunk, 1):
        line = line.strip()
        # LOGID-ë¬¸ìì—´ ì¶”ì¶œ (ì‹œì‘ ë¶€ë¶„)
        if line.startswith("LOGID-"):
            body = line.split(" ", 1)
            logid = body[0]
            rest = body[1] if len(body) > 1 else ""
        else:
            logid = "UNKNOWN-LOGID"
            rest = line
        print(f"{logid} {rest}")
    print("")

def get_elasticsearch_client() -> Optional[Elasticsearch]:
    """
    Elasticsearch í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    
    Returns:
        Elasticsearch: ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ë˜ëŠ” None (ì—°ê²° ì‹¤íŒ¨ì‹œ)
    """
    try:
        client = Elasticsearch(
            [ELASTICSEARCH_HOST],
            basic_auth=(ELASTICSEARCH_USER, ELASTICSEARCH_PASSWORD),
            verify_certs=False,  # ê°œë°œ í™˜ê²½ì—ì„œ SSL ì¸ì¦ì„œ ë¬´ì‹œ
            ssl_show_warn=False
        )
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if client.ping():
            print(f"âœ… Elasticsearch ì—°ê²° ì„±ê³µ: {ELASTICSEARCH_HOST}")
            return client
        else:
            print(f"âŒ Elasticsearch ping ì‹¤íŒ¨: {ELASTICSEARCH_HOST}")
            return None
            
    except ConnectionError as e:
        print(f"âŒ Elasticsearch ì—°ê²° ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"âŒ Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def send_to_elasticsearch(data: Dict[str, Any], log_type: str, chunk_id: Optional[int] = None) -> bool:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ Elasticsearchì— ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        data: ì „ì†¡í•  ë¶„ì„ ë°ì´í„° (JSON í˜•íƒœ)
        log_type: ë¡œê·¸ íƒ€ì… ("httpd_access", "httpd_apache_error", "linux_system")
        chunk_id: ì²­í¬ ë²ˆí˜¸ (ì„ íƒì )
    
    Returns:
        bool: ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    client = get_elasticsearch_client()
    if not client:
        return False
    
    try:
        # ë¬¸ì„œ ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + ë¡œê·¸íƒ€ì… + ì²­í¬ID)
        timestamp = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")
        doc_id = f"{log_type}_{timestamp}"
        if chunk_id is not None:
            doc_id += f"_chunk_{chunk_id}"
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        enriched_data = {
            **data,
            "@timestamp": datetime.datetime.utcnow().isoformat(),
            "log_type": log_type,
            "document_id": doc_id
        }
        
        # Elasticsearchì— ë¬¸ì„œ ì¸ë±ì‹±
        response = client.index(
            index=ELASTICSEARCH_INDEX,
            id=doc_id,
            document=enriched_data
        )
        
        if response.get('result') in ['created', 'updated']:
            print(f"âœ… Elasticsearch ì „ì†¡ ì„±ê³µ: {doc_id}")
            return True
        else:
            print(f"âŒ Elasticsearch ì „ì†¡ ì‹¤íŒ¨: {response}")
            return False
            
    except RequestError as e:
        print(f"âŒ Elasticsearch ìš”ì²­ ì˜¤ë¥˜: {e}")
        return False
    except Exception as e:
        print(f"âŒ Elasticsearch ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# def create_elasticsearch_index_if_not_exists() -> bool:
#     """
#     Elasticsearch ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    
#     Returns:
#         bool: ì¸ë±ìŠ¤ ìƒì„±/í™•ì¸ ì„±ê³µ ì—¬ë¶€
#     """
#     client = get_elasticsearch_client()
#     if not client:
#         return False
    
#     try:
#         # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
#         if client.indices.exists(index=ELASTICSEARCH_INDEX):
#             print(f"âœ… Elasticsearch ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬: {ELASTICSEARCH_INDEX}")
#             return True
        
#         # ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜
#         index_mapping = {
#             "mappings": {
#                 "properties": {
#                     "@timestamp": {"type": "date"},
#                     "chunk_analysis_start_utc": {"type": "date"},
#                     "chunk_analysis_end_utc": {"type": "date"},
#                     "log_type": {"type": "keyword"},
#                     "document_id": {"type": "keyword"},
#                     "summary": {"type": "text", "analyzer": "standard"},
#                     "observations": {"type": "text", "analyzer": "standard"},
#                     "planning": {"type": "text", "analyzer": "standard"},
#                     "highest_severity": {"type": "keyword"},
#                     "requires_immediate_attention": {"type": "boolean"},
#                     "log_hash_mapping": {
#                         "type": "object",
#                         "properties": {
#                             "LOGID-*": {"type": "text", "index": False}
#                         }
#                     },
#                     "events": {
#                         "type": "nested",
#                         "properties": {
#                             "event_type": {"type": "keyword"},
#                             "severity": {"type": "keyword"},
#                             "confidence_score": {"type": "float"},
#                             "requires_human_review": {"type": "boolean"},
#                             "source_ips": {"type": "ip"},
#                             "possible_attack_patterns": {"type": "keyword"}
#                         }
#                     },
#                     "statistics": {"type": "object", "enabled": True}
#                 }
#             },
#             "settings": {
#                 "number_of_shards": 1,
#                 "number_of_replicas": 0
#             }
#         }
        
#         # ì¸ë±ìŠ¤ ìƒì„±
#         response = client.indices.create(
#             index=ELASTICSEARCH_INDEX,
#             body=index_mapping
#         )
        
#         if response.get('acknowledged'):
#             print(f"âœ… Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ: {ELASTICSEARCH_INDEX}")
#             return True
#         else:
#             print(f"âŒ Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {response}")
#             return False
            
#     except RequestError as e:
#         print(f"âŒ Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ìš”ì²­ ì˜¤ë¥˜: {e}")
#         return False
#     except Exception as e:
#         print(f"âŒ Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return False

def generate_log_hash(log_content: str) -> str:
    """
    ë¡œê·¸ ë‚´ìš©ìœ¼ë¡œë¶€í„° í•´ì‹œê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        log_content: ì›ë³¸ ë¡œê·¸ ë‚´ìš©
    
    Returns:
        str: LOGID-{HASH} í˜•íƒœì˜ ë¬¸ìì—´
    """
    import hashlib
    hash_object = hashlib.md5(log_content.encode('utf-8'))
    hash_hex = hash_object.hexdigest()
    return f"LOGID-{hash_hex.upper()}"

def extract_log_content_from_logid_line(logid_line: str) -> tuple[str, str]:
    """
    LOGIDê°€ í¬í•¨ëœ ë¼ì¸ì—ì„œ LOGIDì™€ ì›ë³¸ ë¡œê·¸ ë‚´ìš©ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        logid_line: "LOGID-{HASH} {original_log_content}" í˜•íƒœì˜ ë¬¸ìì—´
    
    Returns:
        tuple: (logid, original_log_content)
    """
    if logid_line.startswith("LOGID-"):
        parts = logid_line.split(" ", 1)
        logid = parts[0]
        original_content = parts[1] if len(parts) > 1 else ""
        return logid, original_content
    else:
        return "UNKNOWN-LOGID", logid_line

def verify_log_hash(logid: str, original_content: str) -> bool:
    """
    LOGIDì˜ í•´ì‹œê°’ì´ ì›ë³¸ ë¡œê·¸ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        logid: LOGID-{HASH} í˜•íƒœì˜ ë¬¸ìì—´
        original_content: ì›ë³¸ ë¡œê·¸ ë‚´ìš©
    
    Returns:
        bool: í•´ì‹œê°’ì´ ì¼ì¹˜í•˜ë©´ True, ì•„ë‹ˆë©´ False
    """
    expected_logid = generate_log_hash(original_content)
    return logid == expected_logid

def create_log_hash_mapping(chunk: list[str]) -> Dict[str, str]:
    """
    ì²­í¬ì˜ ëª¨ë“  ë¡œê·¸ì— ëŒ€í•´ LOGID -> ì›ë³¸ ë¡œê·¸ ë‚´ìš© ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        chunk: LOGIDê°€ í¬í•¨ëœ ë¡œê·¸ ë¼ì¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        Dict[str, str]: {logid: original_content} ë§¤í•‘
    """
    mapping = {}
    for line in chunk:
        logid, original_content = extract_log_content_from_logid_line(line.strip())
        mapping[logid] = original_content
    return mapping

def format_and_send_to_elasticsearch(analysis_data: Dict[str, Any], log_type: str, chunk_id: Optional[int] = None, chunk: Optional[list] = None) -> bool:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ê³  Elasticsearchì— ì „ì†¡í•˜ëŠ” í†µí•© í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    Args:
        analysis_data: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
        log_type: ë¡œê·¸ íƒ€ì… ("httpd_access", "httpd_apache_error", "linux_system")
        chunk_id: ì²­í¬ ë²ˆí˜¸ (ì„ íƒì )
        chunk: ì›ë³¸ ë¡œê·¸ ì²­í¬ (í•´ì‹œ ë§¤í•‘ ìƒì„±ìš©, ì„ íƒì )
    
    Returns:
        bool: ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
    # create_elasticsearch_index_if_not_exists()
    
    # ë¡œê·¸ í•´ì‹œ ë§¤í•‘ ì¶”ê°€ (chunkê°€ ì œê³µëœ ê²½ìš°)
    if chunk:
        log_hash_mapping = create_log_hash_mapping(chunk)
        analysis_data["log_hash_mapping"] = log_hash_mapping
        print(f"ğŸ“ ë¡œê·¸ í•´ì‹œ ë§¤í•‘ {len(log_hash_mapping)}ê°œ í•­ëª© ì¶”ê°€ë¨")
    
    # Elasticsearchì— ì „ì†¡
    return send_to_elasticsearch(analysis_data, log_type, chunk_id)
