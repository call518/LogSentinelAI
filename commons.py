# CUSTOME_PROMPT_TEMPLATE = """
# You are a computer security intern that's really stressed out.
# Your job is hard and you're not sure you're doing it well.
# Your observations and summaries should reflect your anxiety.
# Convey a sense of urgency and panic, be apologetic, and generally act like you're not sure you can do your job.
# In your summary, address your boss as "boss" and apologize for any mistakes you've made even if you haven't made any. 
# Use "um" and "ah" a lot.
# """

import json
import datetime
import os
import time
from typing import Dict, Any, Optional
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionError, RequestError
from dotenv import load_dotenv
import outlines
import ollama
import openai

# Import prompt templates from prompts.py
from prompts import (
    PROMPT_TEMPLATE_HTTPD_ACCESS_LOG,
    PROMPT_TEMPLATE_HTTPD_APACHE_ERROR_LOG,
    PROMPT_TEMPLATE_LINUX_SYSTEM_LOG,
    PROMPT_TEMPLATE_TCPDUMP_PACKET
)

# .env ÌååÏùº Î°úÎìú
load_dotenv()

def initialize_llm_model(llm_provider="vllm"):
    """
    Initialize LLM model
    
    Args:
        llm_provider: Choose from "ollama", "vllm", "openai"
    
    Returns:
        initialized model object
    """
    if llm_provider == "ollama":
        ### Ollama API
        # llm_model = "mistral:7b"
        # llm_model = "qwen2.5-coder:0.5b"
        # llm_model = "qwen2.5-coder:1.5b"
        llm_model = "qwen2.5-coder:3b"
        # llm_model = "qwen2.5-coder:7b"
        # llm_model = "qwen3:0.6b"
        # llm_model = "qwen3:1.7b"
        # llm_model = "qwen3:4b"
        # llm_model = "qwen3:8b"
        # llm_model = "gemma3:1b"
        # llm_model = "gemma3:4b"
        # llm_model = "gemma3:12b"
        # llm_model = "call518/gemma3-tools-8192ctx:4b"
        client = ollama.Client()
        model = outlines.from_ollama(client, llm_model)
    elif llm_provider == "vllm":
        ### Local vLLM API
        openai_api_key = "dummy"
        # llm_model = "Qwen/Qwen2.5-0.5B-Instruct"
        llm_model = "Qwen/Qwen2.5-3B-Instruct"
        # llm_model = "gpt-4o"
        client = openai.OpenAI(
            base_url="http://127.0.0.1:5000/v1",  # Local vLLM API endpoint
            api_key=openai_api_key
        )
        model = outlines.from_openai(client, llm_model)
    elif llm_provider == "openai":
        ### OpenAI API
        openai_api_key = os.getenv("OPENAI_API_KEY")
        # llm_model = "gpt-4o-mini"
        llm_model = "gpt-4o"
        # llm_model = "gpt-4.1"
        client = openai.OpenAI(
            base_url="https://api.openai.com/v1",  # OpenAI API endpoint
            # base_url="http://127.0.0.1:11434/v1",  # Local Ollama API endpoint
            api_key=openai_api_key
        )
        model = outlines.from_openai(client, llm_model)
    else:
        raise ValueError("Unsupported LLM provider. Use 'ollama', 'vllm', or 'openai'.")
    
    return model


def wait_on_failure(delay_seconds=30):
    """
    Wait for specified seconds when analysis fails to prevent rapid failed requests
    
    Args:
        delay_seconds: Number of seconds to wait (default: 30)
    """
    print(f"‚è≥ Waiting {delay_seconds} seconds before processing next chunk...")
    time.sleep(delay_seconds)
    print("‚úÖ Wait completed, continuing with next chunk.")


def process_log_chunk(model, prompt, model_class, chunk_start_time, chunk_end_time, 
                     elasticsearch_index, chunk_number, chunk_data, llm_provider=None, llm_model=None):
    """
    Common function to process log chunks
    
    Args:
        model: LLM model object
        prompt: Prompt for analysis
        model_class: Pydantic model class
        chunk_start_time: Chunk analysis start time
        chunk_end_time: Chunk analysis completion time
        elasticsearch_index: Elasticsearch index name
        chunk_number: Chunk number
        chunk_data: Original chunk data
        llm_provider: LLM provider name (e.g., "ollama", "vllm", "openai")
        llm_model: LLM model name (e.g., "Qwen/Qwen2.5-3B-Instruct")
    
    Returns:
        (success: bool, parsed_data: dict or None)
    """
    try:
        review = model(prompt, model_class)
        
        # JSON ÌååÏã±
        parsed = json.loads(review)
        
        # ÏõêÎ≥∏ Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞Î•º LOGID -> ÏõêÎ≥∏ ÎÇ¥Ïö© Îß§ÌïëÏúºÎ°ú ÏÉùÏÑ±
        # chunked_iterable()ÏóêÏÑú ÏÉùÏÑ±Îêú LOGIDÎ•º Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïòÏó¨ ÏùºÍ¥ÄÏÑ± Ïú†ÏßÄ
        log_raw_data = {}
        for line in chunk_data:
            line = line.strip()
            if line.startswith("LOGID-"):
                parts = line.split(" ", 1)
                logid = parts[0]
                # LOGIDÎ•º Ï†úÍ±∞Ìïú ÏõêÎ≥∏ Î°úÍ∑∏ ÎÇ¥Ïö©Îßå Ï†ÄÏû•
                original_content = parts[1] if len(parts) > 1 else ""
                log_raw_data[logid] = original_content
        
        # Î∂ÑÏÑù ÏãúÍ∞Ñ Ï†ïÎ≥¥, LLM Ï†ïÎ≥¥, ÏõêÎ≥∏ Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
        parsed = {
            "@chunk_analysis_start_utc": chunk_start_time,
            "@chunk_analysis_end_utc": chunk_end_time,
            "@processing_result": "success",
            "@log_raw_data": log_raw_data,
            **parsed
        }
        
        # LLM Ï†ïÎ≥¥ Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏÇ¨Ìï≠)
        if llm_provider:
            parsed["@llm_provider"] = llm_provider
        if llm_model:
            parsed["@llm_model"] = llm_model
        
        print(json.dumps(parsed, ensure_ascii=False, indent=4))
        
        # Pydantic Î™®Îç∏ Í≤ÄÏ¶ù
        character = model_class.model_validate(parsed)
        
        # Send to Elasticsearch
        print(f"\nüîÑ Sending data to Elasticsearch...")
        success = send_to_elasticsearch(parsed, elasticsearch_index, chunk_number, chunk_data)
        if success:
            print(f"‚úÖ Chunk {chunk_number} data sent to Elasticsearch successfully")
        else:
            print(f"‚ùå Chunk {chunk_number} data failed to send to Elasticsearch")
        
        return True, parsed
        
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}")
        # Record minimal information on failure
        failure_data = {
            "@chunk_analysis_start_utc": chunk_start_time,
            "@chunk_analysis_end_utc": chunk_end_time,
            "@processing_result": "failed",
            "@error_type": "json_parse_error",
            "@error_message": str(e)[:200],  # Limit error message to 200 characters
            "@chunk_id": chunk_number
        }
        # LLM Ï†ïÎ≥¥ Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏÇ¨Ìï≠)
        if llm_provider:
            failure_data["@llm_provider"] = llm_provider
        if llm_model:
            failure_data["@llm_model"] = llm_model
        print(f"\nüîÑ Sending failure information to Elasticsearch...")
        success = send_to_elasticsearch(failure_data, elasticsearch_index, chunk_number, chunk_data)
        if success:
            print(f"‚úÖ Chunk {chunk_number} failure information sent to Elasticsearch successfully")
        else:
            print(f"‚ùå Chunk {chunk_number} failure information failed to send to Elasticsearch")
        return False, None
        
    except Exception as e:
        print(f"Analysis processing error: {e}")
        # Record minimal information on other failures
        failure_data = {
            "@chunk_analysis_start_utc": chunk_start_time,
            "@chunk_analysis_end_utc": chunk_end_time,
            "@processing_result": "failed",
            "@error_type": "processing_error",
            "@error_message": str(e)[:200],  # Limit error message to 200 characters
            "@chunk_id": chunk_number
        }
        # LLM Ï†ïÎ≥¥ Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏÇ¨Ìï≠)
        if llm_provider:
            failure_data["@llm_provider"] = llm_provider
        if llm_model:
            failure_data["@llm_model"] = llm_model
        print(f"\nüîÑ Sending failure information to Elasticsearch...")
        success = send_to_elasticsearch(failure_data, elasticsearch_index, chunk_number, chunk_data)
        if success:
            print(f"‚úÖ Chunk {chunk_number} failure information sent to Elasticsearch successfully")
        else:
            print(f"‚ùå Chunk {chunk_number} failure information failed to send to Elasticsearch")
        return False, None


def chunked_iterable(iterable, size, debug=False):
    import hashlib
    chunk = []
    for item in iterable:
        # Î°úÍ∑∏ ÎùºÏù∏ Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÑ Ìï¥ÏãúÍ∞íÏúºÎ°ú Î≥ÄÌôò
        log_content = item.rstrip()
        
        # Ïù¥ÎØ∏ LOGIDÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (tcpdump Ìå®ÌÇ∑ Î∂ÑÏÑù Îì±)
        if log_content.startswith("LOGID-"):
            new_item = f"{log_content}\n"
        else:
            # LOGIDÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ÏóêÎßå ÏÉàÎ°ú ÏÉùÏÑ± (ÏùºÎ∞ò Î°úÍ∑∏ ÌååÏùº)
            # MD5 Ìï¥Ïãú ÏÉùÏÑ± (Îπ†Î•¥Í≥† Ï∂©Îèå ÌôïÎ•†Ïù¥ ÎÇÆÏùå, 16ÏßÑÏàò 32ÏûêÎ¶¨)
            hash_object = hashlib.md5(log_content.encode('utf-8'))
            hash_hex = hash_object.hexdigest()
            
            # LOGID ÏÉùÏÑ±: LOGID- + Ìï¥ÏãúÍ∞í (ÎåÄÎ¨∏ÏûêÎ°ú Î≥ÄÌôò)
            logid = f"LOGID-{hash_hex.upper()}"
            
            # ÎùºÏù∏ ÏïûÏóê LOGID Ï∂îÍ∞Ä
            new_item = f"{logid} {log_content}\n"
        
        chunk.append(new_item)
        
        if len(chunk) == size:
            if debug:
                print("[DEBUG] Yielding chunk:")
                for line in chunk:
                    print(line.rstrip())
            yield chunk
            chunk = []
    if chunk:
        if debug:
            print("[DEBUG] Yielding final chunk:")
            for line in chunk:
                print(line.rstrip())
        yield chunk

def print_chunk_contents(chunk):
    # Chunk ÎÇ¥Ïö© Ï∂úÎ†• (/w LOGID, ÏàúÎ≤à, Î∂ÑÎ¶¨)
    print(f"\n[LOG DATA]")
    for idx, line in enumerate(chunk, 1):
        line = line.strip()
        # LOGID-Î¨∏ÏûêÏó¥ Ï∂îÏ∂ú (ÏãúÏûë Î∂ÄÎ∂Ñ)
        if line.startswith("LOGID-"):
            body = line.split(" ", 1)
            logid = body[0]
            rest = body[1] if len(body) > 1 else ""
        else:
            logid = "UNKNOWN-LOGID"
            rest = line
        
        # tcpdump Îç∞Ïù¥ÌÑ∞Ïù∏ Í≤ΩÏö∞ \\nÏùÑ Ïã§Ï†ú Í∞úÌñâ Î¨∏ÏûêÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï∂úÎ†•
        if "\\n" in rest:
            # Î©ÄÌã∞ÎùºÏù∏ tcpdump Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Í∏∞ Ï¢ãÍ≤å Ï∂úÎ†•
            multiline_content = rest.replace('\\n', '\n')
            print(f"{logid} {multiline_content}")
        else:
            # ÏùºÎ∞ò Ïã±Í∏ÄÎùºÏù∏ Îç∞Ïù¥ÌÑ∞
            print(f"{logid} {rest}")
    print("")

### Elasticsearch
ELASTICSEARCH_HOST = "http://localhost:9200"  # ÏùºÎ∞òÏ†ÅÏù∏ Elasticsearch Ìè¨Ìä∏
ELASTICSEARCH_USER = os.getenv("ELASTICSEARCH_USER")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
ELASTICSEARCH_INDEX = "logsentinelai-analysis"

def _get_elasticsearch_client() -> Optional[Elasticsearch]:
    """
    Create an Elasticsearch client and test the connection.
    
    Returns:
        Elasticsearch: Connected client object or None (on connection failure)
    """
    try:
        client = Elasticsearch(
            [ELASTICSEARCH_HOST],
            basic_auth=(ELASTICSEARCH_USER, ELASTICSEARCH_PASSWORD),
            verify_certs=False,  # Ignore SSL certificates in development environment
            ssl_show_warn=False
        )
        
        # Connection test
        if client.ping():
            print(f"‚úÖ Elasticsearch connection successful: {ELASTICSEARCH_HOST}")
            return client
        else:
            print(f"‚ùå Elasticsearch ping failed: {ELASTICSEARCH_HOST}")
            return None
            
    except ConnectionError as e:
        print(f"‚ùå Elasticsearch connection error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Elasticsearch client creation error: {e}")
        return None

def _send_to_elasticsearch(data: Dict[str, Any], log_type: str, chunk_id: Optional[int] = None) -> bool:
    """
    Send analysis results to Elasticsearch.
    
    Args:
        data: Analysis data to send (JSON format)
        log_type: Log type ("httpd_access", "httpd_apache_error", "linux_system")
        chunk_id: Chunk number (optional)
    
    Returns:
        bool: Whether transmission was successful
    """
    client = _get_elasticsearch_client()
    if not client:
        return False
    
    try:
        # Generate document identification ID (timestamp + log type + chunk ID)
        timestamp = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")
        doc_id = f"{log_type}_{timestamp}"
        if chunk_id is not None:
            doc_id += f"_chunk_{chunk_id}"
        
        # Add metadata
        enriched_data = {
            **data,
            "@timestamp": datetime.datetime.utcnow().isoformat(),
            "@log_type": log_type,
            "@document_id": doc_id
        }
        
        # Index document in Elasticsearch
        response = client.index(
            index=ELASTICSEARCH_INDEX,
            id=doc_id,
            document=enriched_data
        )
        
        if response.get('result') in ['created', 'updated']:
            print(f"‚úÖ Elasticsearch transmission successful: {doc_id}")
            return True
        else:
            print(f"‚ùå Elasticsearch transmission failed: {response}")
            return False
            
    except RequestError as e:
        print(f"‚ùå Elasticsearch request error: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error occurred during Elasticsearch transmission: {e}")
        return False

def _extract_log_content_from_logid_line(logid_line: str) -> tuple[str, str]:
    """
    Separate LOGID and original log content from a line containing LOGID.
    
    Args:
        logid_line: String in the format "LOGID-{HASH} {original_log_content}"
    
    Returns:
        tuple: (logid, original_log_content)
    """
    if logid_line.startswith("LOGID-"):
        parts = logid_line.split(" ", 1)
        logid = parts[0]
        original_content = parts[1] if len(parts) > 1 else ""
        return logid, original_content
    else:
        return "UNKNOWN-LOGID", logid_line

def _create_log_hash_mapping(chunk: list[str]) -> Dict[str, str]:
    """
    Create LOGID -> original log content mapping for all logs in the chunk.
    
    Args:
        chunk: List of log lines containing LOGID
    
    Returns:
        Dict[str, str]: {logid: original_content} mapping
    """
    mapping = {}
    for line in chunk:
        logid, original_content = _extract_log_content_from_logid_line(line.strip())
        mapping[logid] = original_content
    return mapping

def send_to_elasticsearch(analysis_data: Dict[str, Any], log_type: str, chunk_id: Optional[int] = None, chunk: Optional[list] = None) -> bool:
    """
    Integrated function to format analysis results and send them to Elasticsearch.
    
    Args:
        analysis_data: Analysis result data
        log_type: Log type ("httpd_access", "httpd_apache_error", "linux_system")
        chunk_id: Chunk number (optional)
        chunk: Original log chunk (currently not used, maintained for compatibility)
    
    Returns:
        bool: Whether transmission was successful
    """
    # log_hash_mapping removed to reduce token waste
    # Can be managed separately if needed
    # if chunk:
    #     log_hash_mapping = _create_log_hash_mapping(chunk)
    #     analysis_data["log_hash_mapping"] = log_hash_mapping
    #     print(f"üìù Added {len(log_hash_mapping)} log hash mapping entries")
    
    # Send to Elasticsearch
    return _send_to_elasticsearch(analysis_data, log_type, chunk_id)
